<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision Fundamentals: Keypoints, Descriptors & Matching for Robotics</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css">
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --light-bg: #f8f9fa;
            --dark-bg: #2c3e50;
            --success-color: #27ae60;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: var(--light-bg);
        }
        
        .navbar-brand {
            font-weight: 700;
            font-size: 1.5rem;
        }
        
        .hero-section {
            background: linear-gradient(135deg, var(--primary-color) 0%, #1a2530 100%);
            color: white;
            padding: 4rem 0;
            margin-bottom: 2rem;
            border-radius: 0 0 20px 20px;
        }
        
        .section-card {
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.08);
            padding: 2rem;
            margin-bottom: 2.5rem;
            border-left: 5px solid var(--secondary-color);
            transition: transform 0.3s ease;
        }
        
        .section-card:hover {
            transform: translateY(-5px);
        }
        
        .math-formula {
            background-color: #f8f9fa;
            border-left: 4px solid var(--secondary-color);
            padding: 1.5rem;
            margin: 1.5rem 0;
            font-family: 'Courier New', monospace;
            border-radius: 0 8px 8px 0;
        }
        
        .concept-visual {
            background: linear-gradient(to bottom right, #e3f2fd, #f3e5f5);
            border-radius: 10px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border: 1px solid #e0e0e0;
        }
        
        .interactive-demo {
            background-color: #f0f7ff;
            border-radius: 10px;
            padding: 2rem;
            margin: 2rem 0;
            border: 2px dashed var(--secondary-color);
        }
        
        .code-block {
            border-radius: 10px;
            margin: 1.5rem 0;
            overflow: hidden;
        }
        
        .nav-tabs .nav-link.active {
            background-color: var(--secondary-color);
            color: white;
            border-color: var(--secondary-color);
        }
        
        .step-indicator {
            display: inline-block;
            width: 30px;
            height: 30px;
            background-color: var(--secondary-color);
            color: white;
            border-radius: 50%;
            text-align: center;
            line-height: 30px;
            margin-right: 10px;
            font-weight: bold;
        }
        
        .keypoint-visual {
            width: 100%;
            height: 300px;
            background: linear-gradient(45deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 10px;
            position: relative;
            margin: 1.5rem 0;
        }
        
        .keypoint {
            position: absolute;
            width: 15px;
            height: 15px;
            background-color: var(--accent-color);
            border-radius: 50%;
            border: 3px solid white;
            box-shadow: 0 0 10px rgba(0,0,0,0.3);
        }
        
        .descriptor-grid {
            display: grid;
            grid-template-columns: repeat(8, 1fr);
            gap: 2px;
            margin: 1rem 0;
        }
        
        .descriptor-cell {
            width: 100%;
            height: 30px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            border-radius: 3px;
        }
        
        .slam-pipeline {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            margin: 2rem 0;
        }
        
        .pipeline-step {
            flex: 1;
            min-width: 180px;
            margin: 0.5rem;
            text-align: center;
            padding: 1rem;
            background: white;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
        }
        
        .pipeline-arrow {
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--secondary-color);
            font-size: 1.5rem;
        }
        
        .comparison-table {
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
        }
        
        .comparison-table th {
            background-color: var(--primary-color);
            color: white;
        }
        
        footer {
            background-color: var(--dark-bg);
            color: white;
            padding: 2rem 0;
            margin-top: 3rem;
        }
        
        @media (max-width: 768px) {
            .slam-pipeline {
                flex-direction: column;
            }
            
            .pipeline-arrow {
                transform: rotate(90deg);
                margin: 1rem 0;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark sticky-top" style="background-color: var(--primary-color);">
        <div class="container">
            <a class="navbar-brand" href="#">
                <i class="fas fa-robot me-2"></i>CV for Robotics
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="#introduction">Introduction</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#keypoints">Keypoints</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#descriptors">Descriptors</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#matching">Matching</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#slam">SLAM Application</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#code">Code Examples</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero-section">
        <div class="container">
            <div class="row align-items-center">
                <div class="col-lg-8">
                    <h1 class="display-4 fw-bold mb-3">Feature Matching Fundamentals for Robotics</h1>
                    <p class="lead mb-4">A comprehensive guide to Keypoints, Descriptors, and Nearest Neighbor Matching with practical SLAM applications</p>
                    <div class="d-flex flex-wrap gap-2">
                        <span class="badge bg-light text-dark p-2">Computer Vision</span>
                        <span class="badge bg-light text-dark p-2">Feature Detection</span>
                        <span class="badge bg-light text-dark p-2">Robotics</span>
                        <span class="badge bg-light text-dark p-2">SLAM</span>
                        <span class="badge bg-light text-dark p-2">OpenCV</span>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <i class="fas fa-project-diagram" style="font-size: 8rem; opacity: 0.8;"></i>
                </div>
            </div>
        </div>
    </section>

    <!-- Main Content -->
    <div class="container">
        <!-- Introduction -->
        <section id="introduction" class="section-card">
            <h2 class="mb-4"><i class="fas fa-play-circle me-2" style="color: var(--secondary-color);"></i> Introduction to Visual Feature Matching</h2>
            <p>Feature matching is the foundation of many computer vision applications in robotics, enabling machines to recognize scenes, track objects, and navigate environments. This tutorial breaks down the three fundamental components:</p>
            
            <div class="row mt-4">
                <div class="col-md-4 mb-3">
                    <div class="card h-100 text-center">
                        <div class="card-body">
                            <div class="step-indicator">1</div>
                            <h5 class="card-title">Keypoint Detection</h5>
                            <p class="card-text">Finding distinctive locations in images that are invariant to transformations</p>
                        </div>
                    </div>
                </div>
                <div class="col-md-4 mb-3">
                    <div class="card h-100 text-center">
                        <div class="card-body">
                            <div class="step-indicator">2</div>
                            <h5 class="card-title">Descriptor Extraction</h5>
                            <p class="card-text">Creating numerical representations of local image regions around keypoints</p>
                        </div>
                    </div>
                </div>
                <div class="col-md-4 mb-3">
                    <div class="card h-100 text-center">
                        <div class="card-body">
                            <div class="step-indicator">3</div>
                            <h5 class="card-title">Feature Matching</h5>
                            <p class="card-text">Finding correspondences between features using nearest neighbor search</p>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="concept-visual mt-4">
                <h5><i class="fas fa-lightbulb me-2"></i>Simple Analogy</h5>
                <p>Imagine you're in a library looking for a specific book:</p>
                <ul>
                    <li><strong>Keypoint</strong> = The book's unique position on a shelf (like coordinates 12, 7, 3)</li>
                    <li><strong>Descriptor</strong> = The book's title, author, ISBN, and summary</li>
                    <li><strong>Nearest Neighbor</strong> = Finding the most similar book by comparing descriptions</li>
                </ul>
                <p>In robotics, this process allows a robot to recognize the same physical point from different camera viewpoints.</p>
            </div>
        </section>

        <!-- Keypoints Section -->
        <section id="keypoints" class="section-card">
            <h2 class="mb-4"><i class="fas fa-crosshairs me-2" style="color: var(--accent-color);"></i> What are Keypoints?</h2>
            
            <h4 class="mt-4">Formal Definition</h4>
            <p>A <strong>keypoint</strong> (or interest point) is a location in an image that has a well-defined position and can be robustly detected under various image transformations. It consists of:</p>
            <ul>
                <li><strong>Coordinates:</strong> (x, y) position in the image</li>
                <li><strong>Scale:</strong> The size of the region the keypoint represents</li>
                <li><strong>Orientation:</strong> Dominant gradient direction</li>
                <li><strong>Response:</strong> Strength or confidence of the detection</li>
            </ul>
            
            <div class="keypoint-visual" id="keypointVisualization">
                <!-- Keypoints will be visualized here via JavaScript -->
            </div>
            
            <h4 class="mt-4">How Keypoint Detectors Work</h4>
            <p>Keypoint detectors use mathematical operators to find regions that stand out from their surroundings:</p>
            
            <div class="math-formula">
                <h6>Harris Corner Detector Formula</h6>
                <p>The Harris detector measures intensity changes in all directions using the structure tensor:</p>
                <p>$$M = \sum_{x,y} w(x,y) \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix}$$</p>
                <p>Where $I_x$ and $I_y$ are image derivatives, and $w(x,y)$ is a Gaussian window. Corners are identified when both eigenvalues of M are large.</p>
            </div>
            
            <div class="math-formula">
                <h6>FAST (Features from Accelerated Segment Test) Algorithm</h6>
                <p>A pixel $p$ is a corner if there are $n$ contiguous pixels in a circle around $p$ that are all brighter or darker than $p$ by threshold $t$:</p>
                <p>$$|I(p) - I(x)| > t \quad \text{for } x \in \text{Bresenham circle of radius 3}$$</p>
                <p>Typically $n = 9$ or $12$ for balanced speed and accuracy.</p>
            </div>
            
            <div class="comparison-table mt-4">
                <table class="table table-hover">
                    <thead>
                        <tr>
                            <th>Detector</th>
                            <th>Principle</th>
                            <th>Invariance</th>
                            <th>Speed</th>
                            <th>Common Use</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Harris Corner</strong></td>
                            <td>Eigenvalues of gradient matrix</td>
                            <td>Rotation, illumination</td>
                            <td>Medium</td>
                            <td>Traditional applications</td>
                        </tr>
                        <tr>
                            <td><strong>FAST</strong></td>
                            <td>Pixel intensity comparisons</td>
                            <td>Rotation</td>
                            <td>Very Fast</td>
                            <td>Real-time systems</td>
                        </tr>
                        <tr>
                            <td><strong>SIFT</strong></td>
                            <td>Difference of Gaussians</td>
                            <td>Rotation, scale, illumination</td>
                            <td>Slow</td>
                            <td>High-accuracy matching</td>
                        </tr>
                        <tr>
                            <td><strong>ORB</strong></td>
                            <td>oFAST + rBRIEF</td>
                            <td>Rotation, scale</td>
                            <td>Fast</td>
                            <td>Real-time SLAM</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <!-- Descriptors Section -->
        <section id="descriptors" class="section-card">
            <h2 class="mb-4"><i class="fas fa-fingerprint me-2" style="color: var(--secondary-color);"></i> What are Descriptors?</h2>
            
            <p>A <strong>descriptor</strong> is a numerical representation of the image patch surrounding a keypoint. It encodes visual information in a way that is:</p>
            <ul>
                <li><strong>Distinctive:</strong> Different patches should have different descriptors</li>
                <li><strong>Invariant:</strong> Same patch under different conditions should have similar descriptors</li>
                <li><strong>Compact:</strong> Efficient to store and compare</li>
                <li><strong>Robust:</strong> Resistant to noise and minor deformations</li>
            </ul>
            
            <div class="interactive-demo">
                <h5><i class="fas fa-sliders-h me-2"></i>Descriptor Visualization</h5>
                <p>Below is a simplified 8×4 descriptor grid showing intensity patterns (darker = lower value, brighter = higher value):</p>
                <div class="descriptor-grid" id="descriptorGrid">
                    <!-- Generated by JavaScript -->
                </div>
                <div class="mt-3">
                    <label for="descriptorType" class="form-label">Descriptor Type:</label>
                    <select id="descriptorType" class="form-select w-50">
                        <option value="orb">ORB (Binary)</option>
                        <option value="sift">SIFT (Gradient-based)</option>
                        <option value="random">Random Pattern</option>
                    </select>
                </div>
            </div>
            
            <h4 class="mt-4">Descriptor Types and Mathematics</h4>
            
            <div class="math-formula">
                <h6>SIFT (Scale-Invariant Feature Transform) Descriptor</h6>
                <p>SIFT creates a 128-element vector from gradient orientations in 4×4 subregions around the keypoint:</p>
                <p>For each 4×4 subregion, compute gradient magnitude $m(x,y)$ and orientation $\theta(x,y)$:</p>
                <p>$$m(x,y) = \sqrt{(L(x+1,y)-L(x-1,y))^2 + (L(x,y+1)-L(x,y-1))^2}$$</p>
                <p>$$\theta(x,y) = \tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right)$$</p>
                <p>Create an 8-bin orientation histogram for each subregion, resulting in 4×4×8 = 128 dimensions.</p>
            </div>
            
            <div class="math-formula">
                <h6>ORB (Oriented FAST and Rotated BRIEF) Descriptor</h6>
                <p>ORB uses a binary descriptor based on intensity comparisons:</p>
                <p>For a keypoint at orientation $\theta$, define test pairs $(p_i, q_i)$ rotated by $\theta$:</p>
                <p>$$\tau(p; \mathbf{p}, \mathbf{q}) = \sum_{i=1}^{n} 2^{i-1} s(p(\mathbf{p}_i), p(\mathbf{q}_i))$$</p>
                <p>Where $s(a,b) = 1$ if $a < b$, else $0$. The result is a 256-bit binary string (32 bytes).</p>
            </div>
            
            <h5 class="mt-4">Numerical Example: Creating a Simple Descriptor</h5>
            <p>Consider a tiny 3×3 image patch around a keypoint:</p>
            <div class="table-responsive">
                <table class="table table-bordered w-50">
                    <tr><td>45</td><td>50</td><td>48</td></tr>
                    <tr><td>52</td><td>55</td><td>53</td></tr>
                    <tr><td>49</td><td>51</td><td>50</td></tr>
                </table>
            </div>
            <p>A simple 4-element descriptor could be:</p>
            <ol>
                <li>Mean intensity: <code>(45+50+48+52+55+53+49+51+50)/9 = 50.33</code></li>
                <li>Horizontal gradient: <code>(right - left) = (48+53+50)/3 - (45+52+49)/3 = 50.33 - 48.67 = 1.66</code></li>
                <li>Vertical gradient: <code>(bottom - top) = (49+51+50)/3 - (45+50+48)/3 = 50.0 - 47.67 = 2.33</code></li>
                <li>Variance: <code>Σ(pixel - mean)² / 9 = 6.22</code></li>
            </ol>
            <p>Resulting descriptor vector: <code>[50.33, 1.66, 2.33, 6.22]</code></p>
        </section>

        <!-- Nearest Neighbor Matching Section -->
        <section id="matching" class="section-card">
            <h2 class="mb-4"><i class="fas fa-link me-2" style="color: var(--success-color);"></i> Nearest Neighbor Matching</h2>
            
            <p>Once we have descriptors, we need to match them between images. This is typically done using <strong>Nearest Neighbor (NN) search</strong>.</p>
            
            <h4 class="mt-4">Distance Metrics</h4>
            <p>To find "nearest" descriptors, we need a way to measure distance between them:</p>
            
            <div class="math-formula">
                <h6>Euclidean Distance (L2 Norm)</h6>
                <p>For two descriptors $A = [a_1, a_2, ..., a_n]$ and $B = [b_1, b_2, ..., b_n]$:</p>
                <p>$$d_{\text{Euclidean}}(A, B) = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}$$</p>
                <p><strong>Example:</strong> For $A = [2, 5, 1]$ and $B = [4, 3, 2]$:</p>
                <p>$$d = \sqrt{(2-4)^2 + (5-3)^2 + (1-2)^2} = \sqrt{4 + 4 + 1} = \sqrt{9} = 3$$</p>
            </div>
            
            <div class="math-formula">
                <h6>Hamming Distance (for binary descriptors)</h6>
                <p>For binary descriptors (like ORB), count the number of differing bits:</p>
                <p>$$d_{\text{Hamming}}(A, B) = \sum_{i=1}^{n} (a_i \oplus b_i)$$</p>
                <p>Where $\oplus$ is the XOR operation.</p>
                <p><strong>Example:</strong> For binary strings $A = 10110101$ and $B = 10011101$:</p>
                <p>Positions differ at bits 3 and 5, so $d = 2$.</p>
            </div>
            
            <div class="interactive-demo">
                <h5><i class="fas fa-calculator me-2"></i>Matching Example</h5>
                <p>We have 3 database descriptors and 1 query descriptor:</p>
                <div class="row">
                    <div class="col-md-6">
                        <h6>Database Descriptors</h6>
                        <ul>
                            <li>D1: [2.1, 5.3, 1.8]</li>
                            <li>D2: [4.2, 3.1, 2.0]</li>
                            <li>D3: [1.9, 4.8, 1.5]</li>
                        </ul>
                    </div>
                    <div class="col-md-6">
                        <h6>Query Descriptor</h6>
                        <p>Q: [2.0, 5.0, 1.7]</p>
                    </div>
                </div>
                <button class="btn btn-primary mt-2" onclick="calculateDistances()">Calculate Distances</button>
                <div id="distanceResults" class="mt-3"></div>
            </div>
            
            <h4 class="mt-4">Matching Algorithms</h4>
            
            <div class="math-formula">
                <h6>Brute-Force Matching</h6>
                <p>Compare query descriptor with every descriptor in database:</p>
                <p>Complexity: $O(n \cdot m)$ where $n$ = query descriptors, $m$ = database descriptors</p>
                <p>Simple but computationally expensive for large databases.</p>
            </div>
            
            <div class="math-formula">
                <h6>FLANN (Fast Library for Approximate Nearest Neighbors)</h6>
                <p>Uses optimized data structures like k-d trees or hierarchical k-means trees:</p>
                <p>Complexity: $O(\log n)$ for searches after $O(n \log n)$ tree construction</p>
                <p>Trades off exact matches for significant speed improvements.</p>
            </div>
            
            <h5 class="mt-4">Ratio Test for Robust Matching</h5>
            <p>To eliminate ambiguous matches, use Lowe's ratio test:</p>
            <div class="math-formula">
                <p>For query descriptor $Q$, find two nearest neighbors $N_1$ and $N_2$ with distances $d_1$ and $d_2$.</p>
                <p>Accept match if: $$\frac{d_1}{d_2} < \text{threshold (typically 0.7-0.8)}$$</p>
                <p>This ensures the best match is significantly better than the second-best.</p>
            </div>
        </section>

        <!-- SLAM Application -->
        <section id="slam" class="section-card">
            <h2 class="mb-4"><i class="fas fa-map-marked-alt me-2" style="color: var(--accent-color);"></i> Application in Robotic SLAM</h2>
            
            <p><strong>SLAM (Simultaneous Localization and Mapping)</strong> is a fundamental problem in robotics where a robot builds a map of an unknown environment while simultaneously tracking its location within that map.</p>
            
            <h4 class="mt-4">Visual SLAM Pipeline</h4>
            <div class="slam-pipeline">
                <div class="pipeline-step">
                    <div class="step-indicator">1</div>
                    <h6>Frame Capture</h6>
                    <p>Robot captures image from camera</p>
                </div>
                <div class="pipeline-arrow">
                    <i class="fas fa-arrow-right"></i>
                </div>
                <div class="pipeline-step">
                    <div class="step-indicator">2</div>
                    <h6>Feature Extraction</h6>
                    <p>Detect keypoints and compute descriptors</p>
                </div>
                <div class="pipeline-arrow">
                    <i class="fas fa-arrow-right"></i>
                </div>
                <div class="pipeline-step">
                    <div class="step-indicator">3</div>
                    <h6>Feature Matching</h6>
                    <p>Match to previous frame or map features</p>
                </div>
                <div class="pipeline-arrow">
                    <i class="fas fa-arrow-right"></i>
                </div>
                <div class="pipeline-step">
                    <div class="step-indicator">4</div>
                    <h6>Pose Estimation</h6>
                    <p>Calculate robot motion from matches</p>
                </div>
                <div class="pipeline-arrow">
                    <i class="fas fa-arrow-right"></i>
                </div>
                <div class="pipeline-step">
                    <div class="step-indicator">5</div>
                    <h6>Map Update</h6>
                    <p>Add new landmarks to map</p>
                </div>
            </div>
            
            <h4 class="mt-4">Detailed Numerical Example: Robot Localization</h4>
            <p>Let's trace through a simplified example of how a robot uses feature matching to estimate its movement:</p>
            
            <div class="concept-visual">
                <h6><i class="fas fa-robot me-2"></i>Scenario</h6>
                <p>A robot observes a landmark (a corner) at position (x=2.0m, y=1.5m) in its coordinate system at time t=0.</p>
                <p>At time t=1, the robot moves and sees what appears to be the same landmark, but now at (x=1.8m, y=1.7m) in its new coordinate system.</p>
            </div>
            
            <h6 class="mt-3">Step 1: Feature Matching Between Frames</h6>
            <p>The robot detects ORB features in both images:</p>
            <ul>
                <li><strong>Frame 0:</strong> Detects 150 features, stores their descriptors in database</li>
                <li><strong>Frame 1:</strong> Detects 155 features, needs to match them to Frame 0</li>
            </ul>
            
            <h6 class="mt-3">Step 2: Nearest Neighbor Search</h6>
            <p>For each feature in Frame 1, perform NN search in Frame 0 database:</p>
            <div class="table-responsive">
                <table class="table table-bordered">
                    <thead>
                        <tr>
                            <th>Frame 1 Feature</th>
                            <th>Closest Frame 0 Match</th>
                            <th>Distance (Hamming)</th>
                            <th>2nd Closest Match</th>
                            <th>Distance</th>
                            <th>Ratio</th>
                            <th>Accept?</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>F1_1 (Desc: 0xA3F1...)</td>
                            <td>F0_42 (Desc: 0xA3F5...)</td>
                            <td>3</td>
                            <td>F0_87 (Desc: 0xB3F1...)</td>
                            <td>12</td>
                            <td>3/12 = 0.25</td>
                            <td class="text-success">✓ (0.25 < 0.8)</td>
                        </tr>
                        <tr>
                            <td>F1_2 (Desc: 0x4C2A...)</td>
                            <td>F0_91 (Desc: 0x4D2A...)</td>
                            <td>5</td>
                            <td>F0_12 (Desc: 0x4C2B...)</td>
                            <td>6</td>
                            <td>5/6 = 0.83</td>
                            <td class="text-danger">✗ (0.83 > 0.8)</td>
                        </tr>
                        <tr>
                            <td>F1_3 (Desc: 0x9B01...)</td>
                            <td>F0_33 (Desc: 0x9B01...)</td>
                            <td>0</td>
                            <td>F0_67 (Desc: 0x9B81...)</td>
                            <td>2</td>
                            <td>0/2 = 0</td>
                            <td class="text-success">✓ (0 < 0.8)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <p>After ratio test: 120 good matches found from 155 features (77% match rate).</p>
            
            <h6 class="mt-3">Step 3: Motion Estimation (Simplified 2D Case)</h6>
            <p>From matched features, we can estimate robot motion. For a single matched point:</p>
            <div class="math-formula">
                <p>Let $P_0 = (x_0, y_0)$ be landmark position in Frame 0 coordinates.</p>
                <p>Let $P_1 = (x_1, y_1)$ be same landmark in Frame 1 coordinates.</p>
                <p>If robot moved by $(\Delta x, \Delta y)$ and rotated by $\theta$, then:</p>
                <p>$$P_0 = R(\theta) \cdot P_1 + T$$</p>
                <p>Where $R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$ and $T = [\Delta x, \Delta y]^T$.</p>
                <p>With multiple matches, we solve for $\theta$, $\Delta x$, $\Delta y$ that minimize reprojection error.</p>
            </div>
            
            <h6 class="mt-3">Step 4: Numerical Calculation</h6>
            <p>Suppose we have 3 good matches with these pixel coordinates:</p>
            <div class="table-responsive">
                <table class="table table-bordered w-75">
                    <tr><th>Feature</th><th>Frame 0 (pixels)</th><th>Frame 1 (pixels)</th></tr>
                    <tr><td>A</td><td>(100, 150)</td><td>(95, 155)</td></tr>
                    <tr><td>B</td><td>(200, 100)</td><td>(205, 95)</td></tr>
                    <tr><td>C</td><td>(150, 200)</td><td>(145, 205)</td></tr>
                </table>
            </div>
            <p>Using the 8-point algorithm (simplified):</p>
            <ol>
                <li>Calculate centroid of points in each frame</li>
                <li>Normalize points by subtracting centroid</li>
            </ol>
        </section>

        <!-- Code Examples -->
        <section id="code" class="section-card">
            <h2 class="mb-4"><i class="fas fa-code me-2" style="color: var(--secondary-color);"></i> Practical Code Examples</h2>
            
            <ul class="nav nav-tabs" id="codeTabs" role="tablist">
                <li class="nav-item" role="presentation">
                    <button class="nav-link active" id="python-tab" data-bs-toggle="tab" data-bs-target="#python" type="button">Python/OpenCV</button>
                </li>
                <li class="nav-item" role="presentation">
                    <button class="nav-link" id="full-tab" data-bs-toggle="tab" data-bs-target="#full" type="button">Complete SLAM Module</button>
                </li>
                <li class="nav-item" role="presentation">
                    <button class="nav-link" id="math-tab" data-bs-toggle="tab" data-bs-target="#math" type="button">Mathematical Implementation</button>
                </li>
            </ul>
            
            <div class="tab-content mt-3">
                <div class="tab-pane fade show active" id="python" role="tabpanel">
                    <pre><code class="language-python">
import cv2
import numpy as np
from matplotlib import pyplot as plt

class FeatureMatcher:
    def __init__(self, detector_type='ORB'):
        """Initialize feature detector and matcher"""
        if detector_type == 'ORB':
            self.detector = cv2.ORB_create(nfeatures=1000, scaleFactor=1.2, nlevels=8)
            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        elif detector_type == 'SIFT':
            self.detector = cv2.SIFT_create(nfeatures=1000)
            self.matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)
        
        self.detector_type = detector_type
        self.ratio_threshold = 0.75  # Lowe's ratio test threshold
    
    def detect_and_compute(self, image):
        """Detect keypoints and compute descriptors"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        keypoints, descriptors = self.detector.detectAndCompute(gray, None)
        return keypoints, descriptors
    
    def match_features(self, desc1, desc2, method='knn'):
        """Match descriptors using specified method"""
        if method == 'knn':
            # k-Nearest Neighbors with ratio test
            matches = self.matcher.knnMatch(desc1, desc2, k=2)
            
            # Apply ratio test
            good_matches = []
            for m, n in matches:
                if m.distance < self.ratio_threshold * n.distance:
                    good_matches.append(m)
            
            return good_matches
        
        elif method == 'bf':
            # Brute-force matching with cross-check
            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
            matches = bf.match(desc1, desc2)
            matches = sorted(matches, key=lambda x: x.distance)
            return matches
    
    def visualize_matches(self, img1, kp1, img2, kp2, matches):
        """Visualize matched features between two images"""
        # Draw matches
        match_img = cv2.drawMatches(
            img1, kp1, img2, kp2, matches[:50], None,
            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
        )
        
        # Add statistics
        cv2.putText(match_img, f'Matches: {len(matches)}', (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        return match_img

# Example usage
if __name__ == "__main__":
    # Load images
    img1 = cv2.imread('frame1.jpg')
    img2 = cv2.imread('frame2.jpg')
    
    # Initialize matcher
    matcher = FeatureMatcher('ORB')
    
    # Detect features
    kp1, desc1 = matcher.detect_and_compute(img1)
    kp2, desc2 = matcher.detect_and_compute(img2)
    
    print(f"Image 1: {len(kp1)} keypoints")
    print(f"Image 2: {len(kp2)} keypoints")
    
    # Match features
    matches = matcher.match_features(desc1, desc2, method='knn')
    print(f"Good matches: {len(matches)}")
    
    # Visualize
    result = matcher.visualize_matches(img1, kp1, img2, kp2, matches)
    cv2.imshow('Feature Matches', result)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
                    </code></pre>
                </div>
                
                <div class="tab-pane fade" id="full" role="tabpanel">
                    <pre><code class="language-python">
import numpy as np
import cv2
from scipy.spatial import KDTree
from typing import List, Tuple, Optional

class VisualSLAMFeatureTracker:
    """A simplified visual feature tracker for SLAM applications"""
    
    def __init__(self):
        self.orb = cv2.ORB_create(nfeatures=2000)
        self.flann = cv2.FlannBasedMatcher(
            dict(algorithm=6, table_number=6, key_size=12, multi_probe_level=1),
            dict(checks=50)
        )
        
        # Database of past features
        self.feature_database = {
            'keypoints': [],      # List of keypoint objects
            'descriptors': [],    # List of descriptor arrays
            'positions': [],      # 3D positions in world coordinates
            'frames': []          # Frame indices where features were seen
        }
        
        self.frame_count = 0
        self.min_matches_for_tracking = 10
    
    def process_frame(self, frame: np.ndarray) -> dict:
        """Process a new frame and return tracking results"""
        self.frame_count += 1
        
        # 1. Feature detection
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        keypoints, descriptors = self.orb.detectAndCompute(gray, None)
        
        if descriptors is None:
            return {'status': 'no_features', 'matches': 0}
        
        # 2. Convert descriptors to float32 for FLANN
        descriptors_float = descriptors.astype(np.float32)
        
        # 3. Match with previous frame (if exists)
        matches = []
        if len(self.feature_database['descriptors']) > 0:
            # Convert database descriptors to float32
            db_descriptors = np.array(self.feature_database['descriptors']).astype(np.float32)
            
            # kNN matching with k=2 for ratio test
            knn_matches = self.flann.knnMatch(descriptors_float, db_descriptors, k=2)
            
            # Apply Lowe's ratio test
            matches = []
            for match_pair in knn_matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < 0.7 * n.distance:
                        matches.append(m)
        
        # 4. Estimate camera motion (simplified - using essential matrix)
        motion_estimate = None
        if len(matches) >= self.min_matches_for_tracking:
            # Extract matched points
            src_pts = np.float32([keypoints[m.queryIdx].pt for m in matches])
            dst_pts = np.float32([self.feature_database['keypoints'][m.trainIdx].pt 
                                 for m in matches])
            
            # Estimate essential matrix
            E, mask = cv2.findEssentialMat(
                src_pts, dst_pts, 
                focal=1.0, pp=(0, 0),
                method=cv2.RANSAC, prob=0.999, threshold=1.0
            )
            
            if E is not None:
                # Recover pose from essential matrix
                _, R, t, mask = cv2.recoverPose(E, src_pts, dst_pts)
                motion_estimate = {'rotation': R, 'translation': t}
        
        # 5. Update feature database (add new features)
        if len(matches) < 100:  # Add new features if we don't have enough matches
            # Select unmatched keypoints
            matched_indices = set([m.queryIdx for m in matches])
            unmatched_indices = [i for i in range(len(keypoints)) 
                               if i not in matched_indices]
            
            # Add top N unmatched features
            n_to_add = min(50, len(unmatched_indices))
            for i in unmatched_indices[:n_to_add]:
                self.feature_database['keypoints'].append(keypoints[i])
                self.feature_database['descriptors'].append(descriptors[i])
                # Initialize with unknown 3D position
                self.feature_database['positions'].append(None)
                self.feature_database['frames'].append(self.frame_count)
        
        return {
            'status': 'success',
            'matches': len(matches),
            'keypoints': len(keypoints),
            'motion': motion_estimate,
            'database_size': len(self.feature_database['descriptors'])
        }
    
    def triangulate_points(self, points1, points2, P1, P2):
        """Triangulate 3D points from 2D correspondences"""
        points_4d = cv2.triangulatePoints(P1, P2, points1.T, points2.T)
        points_3d = points_4d[:3] / points_4d[3]
        return points_3d.T

# Usage example for robot navigation
if __name__ == "__main__":
    # Simulate robot camera frames
    tracker = VisualSLAMFeatureTracker()
    
    # Simulated camera intrinsic parameters
    K = np.array([[800, 0, 320],
                  [0, 800, 240],
                  [0, 0, 1]])
    
    # Process simulated frames
    for i in range(10):
        # In real application, this would be actual camera frames
        # Here we create synthetic frames for demonstration
        frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # Add some features (simulating scene)
        cv2.circle(frame, (100 + i*10, 100), 5, (255, 0, 0), -1)
        cv2.circle(frame, (200, 150 + i*5), 5, (0, 255, 0), -1)
        
        # Process frame
        result = tracker.process_frame(frame)
        
        print(f"Frame {i}: {result['matches']} matches, "
              f"Database: {result['database_size']} features")
        
        if result['motion']:
            print(f"  Estimated motion: {result['motion']['translation'].flatten()}")
                    </code></pre>
                </div>
                
                <div class="tab-pane fade" id="math" role="tabpanel">
                    <pre><code class="language-python">
import numpy as np
from typing import List, Tuple
from dataclasses import dataclass

@dataclass
class Keypoint:
    """Mathematical representation of a keypoint"""
    x: float          # x-coordinate
    y: float          # y-coordinate
    scale: float      # scale at which keypoint was detected
    orientation: float  # orientation in radians
    response: float   # detector response strength

@dataclass  
class Descriptor:
    """Mathematical representation of a descriptor"""
    vector: np.ndarray  # Feature vector
    keypoint: Keypoint  # Associated keypoint
    type: str           # Descriptor type

class MathematicalFeatureMatcher:
    """Pure mathematical implementation of feature matching algorithms"""
    
    @staticmethod
    def compute_simple_descriptor(image_patch: np.ndarray) -> np.ndarray:
        """
        Compute a simple descriptor from an image patch
        Based on intensity and gradient statistics
        """
        h, w = image_patch.shape
        
        # 1. Intensity histogram (4 bins)
        hist, _ = np.histogram(image_patch, bins=4, range=(0, 255))
        hist = hist / (h * w)  # Normalize
        
        # 2. Gradient magnitude and orientation statistics
        # Compute gradients using Sobel operators
        sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
        sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
        
        # Ensure patch is large enough
        if h > 2 and w > 2:
            grad_x = np.abs(np.convolve(image_patch.flatten(), sobel_x.flatten(), 'valid'))
            grad_y = np.abs(np.convolve(image_patch.flatten(), sobel_y.flatten(), 'valid'))
            grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)
            
            grad_mean = np.mean(grad_magnitude) if len(grad_magnitude) > 0 else 0
            grad_std = np.std(grad_magnitude) if len(grad_magnitude) > 0 else 0
        else:
            grad_mean, grad_std = 0, 0
        
        # 3. Create descriptor vector
        descriptor = np.concatenate([
            hist,                    # 4 elements
            [grad_mean, grad_std],   # 2 elements
            [np.mean(image_patch), np.std(image_patch)]  # 2 elements
        ])
        
        return descriptor
    
    @staticmethod
    def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Compute Euclidean distance between two vectors"""
        return np.sqrt(np.sum((vec1 - vec2) ** 2))
    
    @staticmethod
    def hamming_distance(bits1: np.ndarray, bits2: np.ndarray) -> int:
        """Compute Hamming distance between two binary vectors"""
        return np.sum(bits1 != bits2)
    
    @staticmethod
    def brute_force_nn(query: np.ndarray, database: List[np.ndarray]) -> Tuple[int, float]:
        """
        Brute-force nearest neighbor search
        Returns (index_of_best_match, distance)
        """
        best_idx = -1
        best_dist = float('inf')
        
        for i, db_vec in enumerate(database):
            dist = np.linalg.norm(query - db_vec)  # Euclidean distance
            if dist < best_dist:
                best_dist = dist
                best_idx = i
        
        return best_idx, best_dist
    
    @staticmethod
    def knn_search(query: np.ndarray, database: List[np.ndarray], k: int = 2) -> List[Tuple[int, float]]:
        """
        k-Nearest Neighbors search
        Returns list of (index, distance) for k closest matches
        """
        distances = []
        
        for i, db_vec in enumerate(database):
            dist = np.linalg.norm(query - db_vec)
            distances.append((i, dist))
        
        # Sort by distance and return top k
        distances.sort(key=lambda x: x[1])
        return distances[:k]
    
    @staticmethod
    def lowes_ratio_test(matches: List[Tuple[int, float]], ratio_threshold: float = 0.8) -> bool:
        """
        Apply Lowe's ratio test to determine if match is reliable
        Returns True if match passes the test
        """
        if len(matches) < 2:
            return False
        
        # matches[0] is best match, matches[1] is second best
        d1 = matches[0][1]  # distance to best match
        d2 = matches[1][1]  # distance to second best
        
        return d1 < ratio_threshold * d2

# Example: Mathematical workflow for feature matching
if __name__ == "__main__":
    matcher = MathematicalFeatureMatcher()
    
    # Create synthetic image patches (simulating regions around keypoints)
    patch1 = np.random.randint(0, 255, (16, 16))
    patch2 = np.random.randint(0, 255, (16, 16))
    patch3 = np.random.randint(0, 255, (16, 16))
    
    # Create database patches
    db_patches = [np.random.randint(0, 255, (16, 16)) for _ in range(10)]
    
    # Compute descriptors
    query_desc = matcher.compute_simple_descriptor(patch1)
    database_descs = [matcher.compute_simple_descriptor(patch) for patch in db_patches]
    
    print(f"Query descriptor shape: {query_desc.shape}")
    print(f"Descriptor values: {query_desc}")
    
    # Perform brute-force NN search
    best_idx, best_dist = matcher.brute_force_nn(query_desc, database_descs)
    print(f"\nBrute-force NN result:")
    print(f"  Best match index: {best_idx}")
    print(f"  Distance: {best_dist:.4f}")
    
    # Perform k-NN search
    knn_results = matcher.knn_search(query_desc, database_descs, k=3)
    print(f"\nk-NN results (k=3):")
    for idx, (match_idx, dist) in enumerate(knn_results):
        print(f"  Match {idx}: index={match_idx}, distance={dist:.4f}")
    
    # Apply Lowe's ratio test
    passes_test = matcher.lowes_ratio_test(knn_results, ratio_threshold=0.8)
    print(f"\nLowe's ratio test: {'PASS' if passes_test else 'FAIL'}")
    
    # Demonstrate with binary descriptors (simulated)
    print(f"\n--- Binary Descriptor Example ---")
    binary_desc1 = np.random.randint(0, 2, 256)  # 256-bit binary descriptor
    binary_desc2 = np.random.randint(0, 2, 256)
    
    hamming_dist = matcher.hamming_distance(binary_desc1, binary_desc2)
    print(f"Binary descriptor 1: {binary_desc1[:8]}... (first 8 bits)")
    print(f"Binary descriptor 2: {binary_desc2[:8]}... (first 8 bits)")
    print(f"Hamming distance: {hamming_dist} bits different")
                    </code></pre>
                </div>
            </div>
        </section>

        <!-- Summary and Resources -->
        <section class="section-card">
            <h2 class="mb-4"><i class="fas fa-graduation-cap me-2" style="color: var(--success-color);"></i> Summary and Further Resources</h2>
            
            <div class="row">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-body">
                            <h5 class="card-title">Key Takeaways</h5>
                            <ul>
                                <li><strong>Keypoints</strong> are distinctive, repeatable locations in images</li>
                                <li><strong>Descriptors</strong> encode local appearance into numerical vectors</li>
                                <li><strong>Nearest Neighbor</strong> matching finds correspondences between descriptors</li>
                                <li><strong>Ratio test</strong> improves matching robustness</li>
                                <li>These form the foundation for <strong>Visual SLAM</strong> in robotics</li>
                            </ul>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-body">
                            <h5 class="card-title">Common Challenges & Solutions</h5>
                            <ul>
                                <li><strong>Occlusions:</strong> Use robust estimators (RANSAC)</li>
                                <li><strong>Scale changes:</strong> Use scale-invariant detectors (SIFT, ORB)</li>
                                <li><strong>Lighting changes:</strong> Use illumination-invariant descriptors</li>
                                <li><strong>Computational cost:</strong> Use efficient algorithms (FAST, binary descriptors)</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            
            <h5 class="mt-4">Recommended Resources</h5>
            <div class="list-group">
                <a href="https://docs.opencv.org/" class="list-group-item list-group-item-action">
                    <i class="fas fa-book me-2"></i>OpenCV Documentation - Official tutorials and API reference
                </a>
                <a href="https://www.robots.ox.ac.uk/~vgg/hzbook/" class="list-group-item list-group-item-action">
                    <i class="fas fa-book me-2"></i>Multiple View Geometry - Classic computer vision textbook
                </a>
                <a href="https://github.com/raulmur/ORB_SLAM2" class="list-group-item list-group-item-action">
                    <i class="fab fa-github me-2"></i>ORB-SLAM2 - Complete SLAM implementation
                </a>
                <a href="https://courses.cs.washington.edu/courses/cse455/09wi/" class="list-group-item list-group-item-action">
                    <i class="fas fa-university me-2"></i>UW Computer Vision Course - Free online materials
                </a>
            </div>
            
            <div class="alert alert-info mt-4">
                <h5><i class="fas fa-tasks me-2"></i> Practice Exercises</h5>
                <ol>
                    <li>Implement a simple corner detector using the Harris formula</li>
                    <li>Create your own 16-element descriptor using image statistics</li>
                    <li>Implement brute-force matching and compare with kd-tree acceleration</li>
                    <li>Simulate a robot moving in 2D and use feature matching to estimate its trajectory</li>
                </ol>
            </div>
        </section>
    </div>

    <!-- Footer -->
    <footer class="text-center">
        <div class="container">
            <p class="mb-2">Computer Vision Fundamentals for Robotics | Feature Matching Tutorial</p>
            <p class="mb-0">Created for educational purposes | Complete implementation for GitHub Pages</p>
            <div class="mt-3">
                <a href="#" class="text-light mx-2"><i class="fab fa-github fa-lg"></i></a>
                <a href="#" class="text-light mx-2"><i class="fas fa-book fa-lg"></i></a>
                <a href="#" class="text-light mx-2"><i class="fas fa-code fa-lg"></i></a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        // Initialize syntax highlighting
        hljs.highlightAll();
        
        // Initialize MathJax
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
        
        // Interactive descriptor visualization
        function generateDescriptorGrid(type) {
            const grid = document.getElementById('descriptorGrid');
            grid.innerHTML = '';
            
            let values = [];
            if (type === 'orb') {
                // Binary-like values (0 or 1)
                for (let i = 0; i < 32; i++) {
                    values.push(Math.random() > 0.5 ? 1 : 0);
                }
            } else if (type === 'sift') {
                // Gradient-like values (0-1)
                for (let i = 0; i < 32; i++) {
                    values.push(Math.random());
                }
            } else {
                // Random pattern
                for (let i = 0; i < 32; i++) {
                    values.push(Math.random());
                }
            }
            
            for (let i = 0; i < 32; i++) {
                const cell = document.createElement('div');
                cell.className = 'descriptor-cell';
                
                const value = values[i];
                let displayValue, bgColor;
                
                if (type === 'orb') {
                    displayValue = value;
                    bgColor = value === 1 ? '#3498db' : '#2c3e50';
                    cell.style.color = 'white';
                } else {
                    displayValue = value.toFixed(1);
                    const intensity = Math.floor(value * 255);
                    bgColor = `rgb(${intensity}, ${intensity}, ${intensity})`;
                    cell.style.color = intensity > 127 ? 'black' : 'white';
                }
                
                cell.style.backgroundColor = bgColor;
                cell.textContent = displayValue;
                grid.appendChild(cell);
            }
        }
        
        // Keypoint visualization
        function visualizeKeypoints() {
            const container = document.getElementById('keypointVisualization');
            container.innerHTML = '';
            
            // Create random keypoints
            for (let i = 0; i < 15; i++) {
                const keypoint = document.createElement('div');
                keypoint.className = 'keypoint';
                
                // Random position within container
                const x = Math.random() * 90 + 5; // 5-95%
                const y = Math.random() * 90 + 5; // 5-95%
                
                keypoint.style.left = `${x}%`;
                keypoint.style.top = `${y}%`;
                
                // Add tooltip
                keypoint.title = `Keypoint ${i+1}: (${Math.round(x)}, ${Math.round(y)})`;
                
                // Create orientation indicator
                const orientation = document.createElement('div');
                orientation.style.position = 'absolute';
                orientation.style.width = '20px';
                orientation.style.height = '2px';
                orientation.style.backgroundColor = 'white';
                orientation.style.top = '6px';
                orientation.style.left = '15px';
                orientation.style.transformOrigin = '0 0';
                orientation.style.transform = `rotate(${Math.random() * 360}deg)`;
                keypoint.appendChild(orientation);
                
                container.appendChild(keypoint);
            }
            
            // Add scale circles
            const scales = [10, 20, 30];
            scales.forEach(radius => {
                const circle = document.createElement('div');
                circle.style.position = 'absolute';
                circle.style.width = `${radius * 2}px`;
                circle.style.height = `${radius * 2}px`;
                circle.style.border = '1px dashed rgba(52, 152, 219, 0.5)';
                circle.style.borderRadius = '50%';
                circle.style.top = `calc(50% - ${radius}px)`;
                circle.style.left = `calc(50% - ${radius}px)`;
                container.appendChild(circle);
            });
        }
        
        // Calculate distances for matching example
        function calculateDistances() {
            const query = [2.0, 5.0, 1.7];
            const database = [
                [2.1, 5.3, 1.8],
                [4.2, 3.1, 2.0],
                [1.9, 4.8, 1.5]
            ];
            
            let results = '<h6>Distance Calculations:</h6><ul>';
            
            database.forEach((desc, idx) => {
                // Calculate Euclidean distance
                const distance = Math.sqrt(
                    Math.pow(desc[0] - query[0], 2) +
                    Math.pow(desc[1] - query[1], 2) +
                    Math.pow(desc[2] - query[2], 2)
                );
                
                results += `<li>Distance to D${idx+1}: ${distance.toFixed(3)}</li>`;
            });
            
            results += '</ul>';
            
            // Find nearest neighbor
            const distances = database.map((desc, idx) => {
                return {
                    idx: idx,
                    dist: Math.sqrt(
                        Math.pow(desc[0] - query[0], 2) +
                        Math.pow(desc[1] - query[1], 2) +
                        Math.pow(desc[2] - query[2], 2)
                    )
                };
            });
            
            distances.sort((a, b) => a.dist - b.dist);
            
            results += `<div class="alert alert-success mt-2">
                <strong>Nearest Neighbor:</strong> D${distances[0].idx + 1} 
                (distance: ${distances[0].dist.toFixed(3)})<br>
                <strong>Second Nearest:</strong> D${distances[1].idx + 1} 
                (distance: ${distances[1].dist.toFixed(3)})<br>
                <strong>Ratio:</strong> ${(distances[0].dist / distances[1].dist).toFixed(3)}
            </div>`;
            
            document.getElementById('distanceResults').innerHTML = results;
        }
        
        // Initialize visualizations when page loads
        document.addEventListener('DOMContentLoaded', function() {
            visualizeKeypoints();
            generateDescriptorGrid('orb');
            
            // Update descriptor grid when selector changes
            document.getElementById('descriptorType').addEventListener('change', function(e) {
                generateDescriptorGrid(e.target.value);
            });
            
            // Add smooth scrolling for navigation links
            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
                anchor.addEventListener('click', function(e) {
                    e.preventDefault();
                    const targetId = this.getAttribute('href');
                    if(targetId === '#') return;
                    
                    const targetElement = document.querySelector(targetId);
                    if(targetElement) {
                        window.scrollTo({
                            top: targetElement.offsetTop - 80,
                            behavior: 'smooth'
                        });
                    }
                });
            });
            
            // Add active class to current section in view
            window.addEventListener('scroll', function() {
                const sections = document.querySelectorAll('section.section-card');
                const navLinks = document.querySelectorAll('.navbar-nav .nav-link');
                
                let currentSection = '';
                sections.forEach(section => {
                    const sectionTop = section.offsetTop - 100;
                    const sectionHeight = section.clientHeight;
                    if(window.scrollY >= sectionTop && window.scrollY < sectionTop + sectionHeight) {
                        currentSection = section.getAttribute('id');
                    }
                });
                
                navLinks.forEach(link => {
                    link.classList.remove('active');
                    if(link.getAttribute('href') === '#' + currentSection) {
                        link.classList.add('active');
                    }
                });
            });
        });
    </script>
</body>
</html>
